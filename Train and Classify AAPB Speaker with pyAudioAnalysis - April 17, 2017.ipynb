{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import subprocess\n",
    "import array\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyAudioAnalysis import audioSegmentation as aS\n",
    "from pyAudioAnalysis import audioTrainTest as aT\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from IPython.display import display, Audio\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import get_array_type\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import timeit\n",
    "%matplotlib inline\n",
    "\n",
    "os.chdir('/Volumes/McLaughlin-6TB-1/Dropbox/aapb-hipstas/Model_training_clips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "## Enter Speaker Name Here ##\n",
    "#############################\n",
    "\n",
    "speaker=\"King, Martin Luther\"\n",
    "\n",
    "#############################\n",
    "\n",
    "\n",
    "\n",
    "last_name = speaker.split(', ')[0]\n",
    "\n",
    "print(speaker)\n",
    "print(last_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Loading dictionary of applause classification values\n",
    "\n",
    "applause_labels_dir=\"/Volumes/McLaughlin-6TB-1/Dropbox/aapb-hipstas/Applause_labels/\"\n",
    "\n",
    "applause_filenames=os.listdir(applause_labels_dir)\n",
    "\n",
    "\n",
    "applause_dict={}\n",
    "\n",
    "for filename in applause_filenames:\n",
    "    basename=filename.replace('.applause.csv','')\n",
    "    path=os.path.join(applause_labels_dir,filename)\n",
    "    range_table=[]\n",
    "    with open(path) as csvfile:\n",
    "        spamreader = csv.reader(csvfile)\n",
    "        for start,numeric_id,duration in spamreader:\n",
    "            if float(duration)>2.0:\n",
    "                if (float(start)-1.0) > 0:\n",
    "                    adjusted_start=float(start)-1.0\n",
    "                else:\n",
    "                    adjusted_start=float(start)\n",
    "                range_table.append([adjusted_start,float(start)+float(duration)])\n",
    "    applause_dict[basename]=range_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applause_dict['cpb-aacip-15-5m6251fq65__barcode349720_.h264']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_between(value,range_pair):\n",
    "    if float(range_pair[0]) < float(value) < float(range_pair[1]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def spans_overlap(range_1,range_2):\n",
    "    if is_between(range_2[0],range_1):\n",
    "        return True\n",
    "    elif is_between(range_2[1],range_1):\n",
    "        return True\n",
    "    elif is_between(range_1[0],range_2):\n",
    "        return True\n",
    "    elif is_between(range_1[1],range_2):\n",
    "        return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_1=[1725.0, 1738.0]\n",
    "range_2=[1727.0, 1728.0]\n",
    "range_3=[900.0, 901.0]\n",
    "\n",
    "print(spans_overlap(range_1,range_2))\n",
    "print(spans_overlap(range_2,range_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading table of labeled 2-second audio segments\n",
    "\n",
    "aapb_metadata_all_split=pd.read_csv(\"/Volumes/McLaughlin-6TB-1/Dropbox/aapb-hipstas/AAPB_ARLO_All_170414_2_sec_segs.csv\")\n",
    "aapb_metadata_all_split.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function for extracting speaker clips\n",
    "\n",
    "def extract_clips(x_table):\n",
    "    for unique_audio_path in sorted(list(set(list(x_table['Pathname'])))):\n",
    "        inputfile=unique_audio_path\n",
    "        if os.path.exists(inputfile):\n",
    "            if inputfile.lower()[-4:].lower() in ('.wav','.mp3','.mp4'):\n",
    "                wav_source=True\n",
    "                if inputfile.lower()[-4:]=='.mp4':     # Creates a temporary WAV\n",
    "                    wav_source=False                         # if input is MP4\n",
    "                    temp_filename=inputfile.split('/')[-1]+'_temp.wav'\n",
    "                    audio_path='/var/tmp/'+temp_filename   # Pathname for temp WAV\n",
    "                    subprocess.call(['ffmpeg', '-y', '-i', inputfile, audio_path]) # '-y' option overwrites existing file if present\n",
    "                else:\n",
    "                    audio_path=inputfile\n",
    "\n",
    "                song=None\n",
    "                from pydub import AudioSegment\n",
    "                \n",
    "                if inputfile[-4:].lower()=='.mp3':\n",
    "                    song = AudioSegment.from_mp3(audio_path)\n",
    "                else:\n",
    "                    song = AudioSegment.from_wav(audio_path)\n",
    "\n",
    "\n",
    "        for index, rowref in x_table[x_table['Pathname']==unique_audio_path].iterrows():\n",
    "            row=aapb_metadata_all_split.iloc[index] ## Ref to master split table\n",
    "            time_in=row['Timecode IN']\n",
    "            clip_duration=row['Tag Duration']\n",
    "            inputfile=row['Pathname']\n",
    "            basename=row['Filename']\n",
    "            dir_name=row['Value']\n",
    "\n",
    "            \n",
    "            try: os.mkdir(dir_name)\n",
    "            except: pass\n",
    "\n",
    "            applause_table = applause_dict[basename]\n",
    "            \n",
    "            contains_applause=False\n",
    "            \n",
    "            span_pair=[float(time_in),float(time_in)+float(clip_duration)]\n",
    "            \n",
    "            for row in applause_table:\n",
    "                if spans_overlap(row,span_pair):\n",
    "                    contains_applause=True\n",
    "            \n",
    "            if contains_applause==False:\n",
    "                from pydub import AudioSegment \n",
    "                start_msec = float(time_in) * 1000.0\n",
    "                duration_msec = float(clip_duration) * 1000\n",
    "                clip_pathname=basename+'.start_'+str(time_in)[:8]+'.dur_2s.wav'\n",
    "                if not os.path.exists(os.path.join(dir_name,clip_pathname)):\n",
    "                    clip_data = song[start_msec:start_msec+duration_msec]\n",
    "                    clip_data=clip_data.set_channels(1)\n",
    "                    clip_data.export(os.path.join(dir_name,clip_pathname), format=\"wav\")\n",
    "\n",
    "        try:\n",
    "            if wav_source==False:\n",
    "                os.remove(audio_path)\n",
    "        except: pass\n",
    "\n",
    "    print(\"*** All segments extracted! ***\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(aapb_metadata_all_split[aapb_metadata_all_split['Value']==speaker]))+\" clips available for \"+speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Running the Random Clip Extractor #####\n",
    "\n",
    "\n",
    "num_clips=3000     ## number of 2-second clips desired\n",
    "\n",
    "x_table=aapb_metadata_all_split[aapb_metadata_all_split['Value']==speaker].sample(n=num_clips)\n",
    "\n",
    "extract_clips(x_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating ID string appended to each model's filename ##\n",
    "\n",
    "clip_count=len(os.listdir(speaker))\n",
    "\n",
    "model_id = '_'+last_name+'_UBM_'+str(clip_count)+'x2s'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Assembling UBM files\n",
    "\n",
    "os.chdir('/Volumes/McLaughlin-6TB-1/Dropbox/aapb-hipstas/Model_training_clips')\n",
    "\n",
    "new_ubm_dir='UBM'+'_'+last_name\n",
    "\n",
    "\n",
    "try: os.mkdir(new_ubm_dir)\n",
    "except: pass\n",
    "\n",
    "ubm_source=\"/Volumes/U/AAPB_Corpus_May_2017/test_set_616_clips\"\n",
    "ubm_files=[item for item in os.listdir(ubm_source) if (speaker not in item)&('16000.wav' in item)]\n",
    "ubm_files=random.sample(ubm_files,60)\n",
    "for filename in ubm_files:\n",
    "    shutil.copy(os.path.join(ubm_source,filename),new_ubm_dir)\n",
    "\n",
    "    \n",
    "ubm_source=\"/Volumes/U/AAPB_Corpus_May_2017/PennSound_UBM_for_Creeley_full_clips_16000\"\n",
    "ubm_files=[item for item in os.listdir(ubm_source) if (speaker not in item)&('.wav' in item)]\n",
    "ubm_files=random.sample(ubm_files,60)\n",
    "for filename in ubm_files:\n",
    "    shutil.copy(os.path.join(ubm_source,filename),new_ubm_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Train Model\n",
    "import timeit\n",
    "tic=timeit.default_timer()\n",
    "#print(timeit.default_timer() - tic)\n",
    "\n",
    "\n",
    "\n",
    "os.chdir('/Volumes/McLaughlin-6TB-1/Dropbox/aapb-hipstas/Model_training_clips')\n",
    "\n",
    "\n",
    "aT.featureAndTrain([new_ubm_dir,speaker], 1.0, 1.0, aT.shortTermWindow, aT.shortTermStep, \"svm\", \"svm\"+model_id, False)\n",
    "print(\"done\")\n",
    "print(timeit.default_timer() - tic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timeit.default_timer() - tic)/60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Other classifiers ##\n",
    "\n",
    "#os.chdir('/Volumes/McLaughlin-6TB-1/Dropbox/aapb-hipstas/Model_training_clips')\n",
    "\n",
    "#tic=timeit.default_timer()\n",
    "#aT.featureAndTrain([new_ubm_dir,speaker], 1.0, 1.0, aT.shortTermWindow, aT.shortTermStep, \"gradientboosting\", \"gradientboosting\"+model_id, False)\n",
    "#print(\"done\")\n",
    "#print(timeit.default_timer() - tic)\n",
    "#tic=timeit.default_timer()\n",
    "#aT.featureAndTrain([new_ubm_dir,speaker], 1.0, 1.0, aT.shortTermWindow, aT.shortTermStep, \"extratrees\", \"extratrees\"+model_id, False)\n",
    "#print(\"done\")\n",
    "#print(timeit.default_timer() - tic)\n",
    "#tic=timeit.default_timer()\n",
    "#aT.featureAndTrain([new_ubm_dir,speaker], 1.0, 1.0, aT.shortTermWindow, aT.shortTermStep, \"randomforest\", \"randomforest\"+model_id, False)\n",
    "#print(\"done\")\n",
    "#print(timeit.default_timer() - tic)\n",
    "#tic=timeit.default_timer()\n",
    "#aT.featureAndTrain([new_ubm_dir,speaker], 1.0, 1.0, aT.shortTermWindow, aT.shortTermStep, \"knn\", \"knn\"+model_id, False)\n",
    "#print(\"done\")\n",
    "#print(timeit.default_timer() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(timeit.default_timer() - tic)/60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes list of 1-second segments classified as speaker (1.0) or \n",
    "# non-speaker (0.0) and returns list of 2-tuples specifying speaker ranges.\n",
    "# Add 1 to 2nd speaker in each 2-tuple for inclusive time span.\n",
    "def seconds_list_to_ranges(seconds_list): \n",
    "    ranges = []                \n",
    "    for k, g in groupby(enumerate(seconds_list), lambda (i,x):i-x):\n",
    "        group = map(itemgetter(1), g)\n",
    "        ranges.append((group[0], group[-1]))\n",
    "    return ranges\n",
    "\n",
    "\n",
    "# Displays specified audio segment in Jupyter window using IPython.display\n",
    "def display_clip(wav_path,start_time,end_time):\n",
    "    track_data = AudioSegment.from_wav(file=wav_path)\n",
    "    track_data = track_data.set_channels(1)\n",
    "    bit_depth = track_data.sample_width * 8\n",
    "    clip_data = track_data[int(1000*start_time):int(1000*end_time)]\n",
    "    array_type = get_array_type(bit_depth)\n",
    "    numeric_array = array.array(array_type, clip_data._data)\n",
    "    display(Audio(numeric_array, rate=track_data.frame_rate))\n",
    "\n",
    "# Classifies audio at 1-second resolution, plots results if speaker found, \n",
    "# and returns speaker ranges as list of 2-tuples.\n",
    "# Add 1 to 2nd speaker in each 2-tuple for inclusive time span.\n",
    "\n",
    "def find_speaker(audio_path,classifier_model_path):\n",
    "    classifier_model_name = classifier_model_path.split('/')[-1]\n",
    "    classifier_model_type = classifier_model_name.split('_')[0].lower() # assuming model file begins svm_etc\n",
    "    \n",
    "    buffer_secs=0\n",
    "    is_mp3=False\n",
    "    if audio_path.lower()[-4:] in ['.mp3','.mp4','.wav']:    # Creates a temporary WAV\n",
    "        is_mp3=True                        # if input is MP3\n",
    "        random.seed(audio_path)\n",
    "        wav_path='/var/tmp/'+str(random.random())+'_temp.wav' # Filename for temp WAV is a random float\n",
    "        subprocess.call(['ffmpeg', '-i', audio_path, '-y', '-ar', '16000', '-ac', '1', '-af', \"volume=0.99,highpass=f=150, lowpass=f=5000\",  wav_path]) # '-y' option overwrites existing file if present\n",
    "    else:\n",
    "        wav_path=audio_path\n",
    "    print(wav_path)\n",
    "    output, classesAll, acc, CM = aS.mtFileClassification(wav_path, classifier_model_path, classifier_model_type) #or replace with 'svm' etc. as needed\n",
    "    output = list(output)\n",
    "    counter=0\n",
    "    speaker_secs=[]\n",
    "    for speaker in output:\n",
    "        if speaker>0.0:\n",
    "            speaker_secs.append(counter)\n",
    "        counter+=1\n",
    "    speaker_ranges=seconds_list_to_ranges(speaker_secs)\n",
    "    #if len(speaker_ranges)>0:\n",
    "    #    print speaker_ranges\n",
    "    #    print '\\n'\n",
    "    #    pd.Series(output).plot()                      # uncomment to display plot and audio clips in notebook\n",
    "    #    plt.show()\n",
    "    #for pair in speaker_ranges:\n",
    "    #    print pair\n",
    "    #    display_clip(wav_path,pair[0],pair[1]+1)\n",
    "    if is_mp3==True:\n",
    "        os.remove(wav_path)\n",
    "    outputfile=audio_path.split('/')[-1][:-4]+\"_\"+classifier_model_name+\".csv\"\n",
    "    with open(outputfile, 'w') as csv_fo:\n",
    "        speaker_ranges_expanded=[(start,1,end-start+1) for start,end in speaker_ranges]\n",
    "        csv_writer = csv.writer(csv_fo)\n",
    "        csv_writer.writerows(speaker_ranges_expanded)\n",
    "    print(outputfile)\n",
    "    return speaker_ranges_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dir=\"/Volumes/McLaughlin-6TB-1/Dropbox/aapb-hipstas/YouTube_corpus/\"+speaker\n",
    "\n",
    "test_files=[os.path.join(test_dir,item) for item in os.listdir(test_dir) \\\n",
    "            if (\".DS_Store\" not in item)&(item[-4:].lower() in ('.mp4','.mp3','.wav'))]\n",
    "\n",
    "print(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_model_paths=[\"/Volumes/McLaughlin-6TB-1/Dropbox/aapb-hipstas/Model_training_clips/\"+\"svm\"+model_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example speaker search\n",
    "import timeit\n",
    "tic=timeit.default_timer()\n",
    "#print(timeit.default_timer() - tic)\n",
    "\n",
    "#audio_path=test_files[2]\n",
    "\n",
    "for classifier_model_path in classifier_model_paths:\n",
    "    for audio_path in test_files:\n",
    "        try: \n",
    "            dd=find_speaker(audio_path,classifier_model_path)\n",
    "            print(timeit.default_timer() - tic)\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timeit.default_timer() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
